---
title: "DT Lab"
author: "Ruth Efrem"
date: "11/10/21"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Libraries
```{r}
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
library(caret)
library(C50) #Need this to pass into caret 
library(mlbench)
```

Congrats! You just graduated UVA's MSDS program and got a job working at the 
Treasury Department. In partnership with Congress members the Treasury has been 
ask to come up with innovative ways to create tax policy. In doing so they 
want to be able to reliably predict whether American workers make more than 
$50,000 a year and also which variables seem to be most contributing 
to predicting this outcome. 

You would like to be able to explain the model to the mere mortals 
around you but need a fairly robust and flexible approach so you've 
chosen to use decision trees to get started and will possibly move 
to a ensemble model if needed. 

In doing so, similar to  great data scientists of the past 
you remembered the excellent education provided 
to you at UVA in a undergrad data science course and have outline 
20ish steps that will need to be undertaken to complete this task 
(you can add more or combine if needed).  As always, you will need 
to make sure to #comment your work heavily. 


```{r}
#url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
#xx <- readr::read_csv(url)
#View(xx)
```

 Footnotes: 
-	You can add or combine steps if needed
-	Also, remember to try several methods during evaluation and always be 
mindful of how the model will be used in practice.
- Make sure all your variables are the correct type (factor, character, etc.)


```{r}
#1 Load the data, check for missing data and ensure the labels are correct. 
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
xx <- readr::read_csv(url)
View(xx)
xx <- readr::read_csv(url, col_names = FALSE)
names <- c("age","workclass","fnlwgt","education","education-num","marital-status","occupation","relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country", "salary")
names(xx) <- names
View(xx)
```

```{r}
#2 Ensure all the variables are classified correctly including the target 
# variable
str(xx)
xx$salary = as.factor(xx$salary)
xx$workclass = as.factor(xx$workclass)
xx$sex = as.factor(xx$sex)
xx$race = as.factor(xx$race)
xx$`native-country`= as.factor(xx$`native-country`)
xx$relationship= as.factor(xx$relationship)
xx$`marital-status`= as.factor(xx$`marital-status`)
xx$education = as.factor(xx$education)
xx = xx[-c(2,4,7,14)]
View(xx)
str(xx)
table(xx$salary)

```
The data is very imbalanced, with much more people making greater than or equal to 50,000 dollars than people making less than 50,000 dollars

```{r}
#3 Don't check for correlated variables....because it doesn't matter with 
# Decision Trees...the make local greedy decisions. 
```

```{r}
#4 Guess what, you also don't need to standardize the data, 
#because DTs don't give a ish, they make local decisions...keeps getting easier
```

```{r}
#5 Determine the baserate or prevalence for the classifier, 
# what does this number mean?
```

According to the prevalence given further into this code in the confusion matrix, the prevalence is 0.76. Prevalence gives us an indication of the balance of the data set. This means that 76 percent of people are making less than 50,000 dollars a year and 24 percent are making more than or equal to 50,000 dollars per year.

```{r}
#6 Split your data into test, tune, and train. (70/15/15)
#There is not a easy way to create 3 partitions using the createDataPartitions
#so we are going to use it twice. Mostly because we want to stratify on the variable we are working to predict. What does that mean?  
part_index_1 <- caret::createDataPartition(xx$salary,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- xx[part_index_1, ]
tune_and_test <- xx[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$salary,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(test) 
dim(tune)
```

```{r}
#7 Build your model using the training data and default settings in caret, 
# double check to make sure you are using a cross-validation training approach
# Choose the features and classes
features <- train[,c(-2,-8:-11)]#dropping these columns which includes target variable, salary
target <- train$salary
View(target)
str(features)
str(target)
#Cross validation process 
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all",
                          #classProbs = TRUE,
                          allowParallel = TRUE) 
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats
# Grid search options for each of the models available in CARET
# http://topepo.github.io/caret/train-models-by-tag.html#tree-based-model
grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,15,20), 
                    .model="tree")
#expand.grid - series of options that are available for model training
#winnow - whether to reduce the feature space -  Works to remove unimportant 
#features but it doesn't always work, in the above we are winnowing (process of eliminating variable based off their contribution to the model.  
#Actually a pretty good StackExchange post on winnowing:
#https://stats.stackexchange.com/questions/83913/understanding-the-output-of-c5-0-classification-model-using-the-caret-package
#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model
set.seed(1984)
xx_mdl <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid,
                trControl=fitControl,
                verbose=TRUE)
xx_mdl #provides us the hyper-parameters that were selected through the grid
```

```{r}
#8 View the results, what is the most important variable for the tree?
varImp(xx_mdl)
```

The variables : education-num and relationship are the best ones to use and both overall are 100.00

```{r}
#9 Plot the output of the model to see the tree visually 
xx_mdl #provides us the hyper-parameters that were selected through the grid
# search process. 
#View(xx_mdl$pred)
# visualize the re-sample distributions
xyplot(xx_mdl,type = c("g", "p", "smooth"))
```



```{r}
#10 Use the validation (test) set and the predict function with your model to predict the target
#11 Compare the predicted values to those of the actual by generating a 
# matrix ("by-hand").
xx_eval <-(predict(xx_mdl,newdata = test))#generates 2 salary categories
View(xx_eval)
xx_eval_prob <- predict(xx_mdl, newdata = test, type = "prob")#this gives us the predicted prob, we will need these later for the fairness evaluation
View(xx_eval_prob)
View(test$salary)
table(xx_eval, test$salary)
xx_eval_prob$test <- test$salary
View(xx_eval_prob)
(error = mean(xx_eval != test$salary))
# 16.85 percent error
```


```{r}
#12 Use the the confusion matrix function to check a variety of metrics 
# and comment on the metric that might be best for this type of analysis given your question.  
xx_pred_tune = predict(xx_mdl,tune, type= "raw")
View(as_tibble(xx_pred_tune))
#Lets use the confusion matrix
(xx_eval <- confusionMatrix(as.factor(xx_pred_tune), 
                as.factor(tune$salary), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))
table(tune$salary)
(xx_pred_tune_p = predict(xx_mdl,tune,type= "prob"))
```

Because the data is extremely imbalanced, accuracy would not be a good evaluation metric as there are significantly more people earning more than or equal to 50,000 dollars per year as opposed to people making less than 50,000 dollars. This makes it more prone to predicting the majority class correctly, but the minority class less often. A good metric would be ROC AUC or F1. AUC ROC is particularly helpful at indicating how well the probabilities from the positive class are separated from the negative class. However, AUC ROC may be a bit too optimistic with imbalanced data sets, so F1 score may be ideal as it takes false positives and false negatives into account. 


```{r}
#13 Generate a ROC and AUC output, interpret the results

library(ROCR)
xx_pred_tune = predict(xx_mdl,tune, type= "raw")
View(as_tibble(xx_pred_tune))
table(tune$salary)
xx_eval <- tibble(pred_class= xx_pred_tune, pred_prob = xx_pred_tune_p$`>50K`,target=as.numeric(test$salary))
View(xx_eval)



salaryx = tune$salary
xx_roc = roc(salaryx, as.numeric(xx_pred_tune_p$`>50K`, plot = TRUE))
xx_roc #area under curve = 0.845
plot(xx_roc)
```

This model is doing particularly well. The higher the AUC, the better the better the model is at distinguishing between the positive and negative classes. The area under the curve is 0.845, so it has moderate classification accuracy of the salaries.

```{r}
#15 Based on your understanding of the model and data adjust several of the hyper-parameters via the built in train control function in caret or build and try new features, does the model quality improve? If so how and why, if not, why not?
# Use this link: https://rdrr.io/cran/caret/man/trainControl.html to select changes,
# you aren't expected to understand all these options but explore one or two and 
# see what happens. 

fitControl_new <- trainControl(method = "repeatedcv",
                          number = 2,
                          seeds = NA,
                          repeats = 2, 
                          returnResamp="all",
                          #classProbs = TRUE,
                          allowParallel = TRUE) 
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats
# Grid search options for each of the models available in CARET
# http://topepo.github.io/caret/train-models-by-tag.html#tree-based-model
grid_new <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,30,20), 
                    .model="tree")
#expand.grid - series of options that are available for model training
#winnow - whether to reduce the feature space -  Works to remove unimportant 
#features but it doesn't always work, in the above we are winnowing (process of eliminating variable based off their contribution to the model.  
#Actually a pretty good StackExchange post on winnowing:
#https://stats.stackexchange.com/questions/83913/understanding-the-output-of-c5-0-classification-model-using-the-caret-package
#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model
set.seed(1984)
xx_mdl2 <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid_new,
                trControl=fitControl_new,
                verbose=TRUE)
xx_mdl2

(xx_pred_tune_p2 = predict(xx_mdl2,tune,type= "prob"))

xx_pred_tune2 = predict(xx_mdl2,tune, type= "raw")
View(as_tibble(xx_pred_tune2))
table(tune$salary)
xx_eval <- tibble(pred_class= xx_pred_tune2, pred_prob = xx_pred_tune_p2$`>50K`,target=as.numeric(test$salary))
View(xx_eval2)



salaryx = tune$salary
xx_roc = roc(salaryx, as.numeric(xx_pred_tune_p2$`>50K`, plot = TRUE))
xx_roc #area under curve = 0.866
plot(xx_roc)


```

The model does not improve in terms of the error rate. The error rate increased meaning the model became more prone to miss-classifying the salaries. I added seeds = NA and decreased the number of repeats and numbers. Decreasing the number of repeats would be decreasing the re-sampling iterations, which would prevent further assessment of the learning algorithm, therefore decreasing accuracy. I do not believe adding the "seeds= NA" did much to the initial model. Interestingly enough, the AUC ROC curve is higher with the altered hyper-parameters, which leads me to believe that there is a higher ratio of false positive to false negatives, which doesn't entirely mean that the second model is better.

```{r}
#16 Once you are confident that your model is not improving, via changes 
# implemented on the training set and evaluated on the the validation set (item 16), predict with the test set and report a final evaluation of the model. Discuss the output in comparison with the previous evaluations. 


xx_eval2 <-(predict(xx_mdl2,newdata = test))#generates 2 salary categories
View(xx_eval2)
xx_eval_prob2 <- predict(xx_mdl2, newdata = test, type = "prob")#this gives us the predicted prob, we will need these later for the fairness evaluation
View(xx_eval_prob2)
table(xx_eval2, test$salary)
xx_eval_prob2$test <- test$salary
View(xx_eval_prob2)
(error = mean(xx_eval2 != test$salary))

#17.5 percent error rate

```

Overall, I believe the decision tree model has good classification accuracy and is not improving with the hyper-parameters that I added and/or changed as there is an increase form 16.8 to 17.5 percent error rate, therefore it is not as good of a model- it is getting worse. The ROC AUC model evaluation metric is improving as it goes from 0.845 to 0.866, however that only really tells us the ratio of false positives to negatives, so that does not necessarily mean that that it is a better model.

```{r}
#17 Summarize what you learned along the way and make recommendations on how this could be used moving forward, being careful not to over promise. -do not suggest model can do more than it can
```

Overall, I learned the general process of how decision trees are used to classify a certain variable within the provided data set from class. I cleaned the data and converted all the characters to factors. I examined the prevalence and discovered the major imbalance of people in terms of them making below or above 50,000 dollars. After training, testing, and tuning, I learned more about the cross validation process and how that evaluates and compares machine learning algorithms and splits them into two segments. I learned about the most important variables, education-num and relationship by utilizing the varImp() function which was very interesting to discover about the data because these are the factors that are major indicators of the salary. Then I created an AUC ROC curve to evaluate the model and created a new model with adjusted hyper-parameters to compare it with my original model. Ideally, this model is able to be used to determine the salary of a certain person based off of certain patterns found in the features I selected (ie. race, education, etc). This data and the results can lead to further studies that examine the association between certain factors like age or race and how and why that can potentially correlate with income. 



```{r}
#18 What was the most interesting or hardest part of this process and what questions do you still have? 
```

I feel like the hardest part was generating hyper-parameters that worked/actually changed the way the model classified the salaries. My model is also still very slow when I run it, so having to re-run the model with different parameters several times and having to wait a long time in between each was not ideal. There is definitely a more efficient way to do so, however I worked with what I knew and eventually got there, even if it was not entirely efficient in terms of time. I am wondering what can I do to prevent the model from being so slow when I run it.



#knitted html and threshold chunk not needed 


